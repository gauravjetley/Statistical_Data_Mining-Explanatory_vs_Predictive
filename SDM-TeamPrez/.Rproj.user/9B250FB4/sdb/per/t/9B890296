{
    "contents" : "#Setting working dir\nsetwd(\"C:/Users/Gaurav Jetley/Desktop/Kaggle Competitions/Titanic\")\n\n\n#Importing test.csv and viewing it\ntest <- read.csv(\"C:/Users/Gaurav Jetley/Desktop/Kaggle Competitions/Titanic/test.csv\")\n   View(test)\n\n#To import the dataframe with string instead of factors, use \n#read.csv(\"location\", stringsAsFactors=FALSE)\n\n#Importing train.csv and Viewing it\ntrain <- read.csv(\"C:/Users/Gaurav Jetley/Desktop/Kaggle Competitions/Titanic/train.csv\")\n  View(train)\n\n#Most basic 'summery statistics' command which shows the ouccourances of each value\ntable(train$Survived)\n\n#Proportion command\nprop.table(table(train$Survived))\n\n#Creating 'Survived' column in test.csv and populating it with 0's(didn't survive)\ntest$Survived <- rep(0,418)\n\n#Creating new .csv file (a test which we need to submit to kaggle) with Passenger ID &\n#Survived columns\n#The file predicts that all passengers died!!\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)\nwrite.csv(submit, file = \"theyallperish.csv\", row.names=FALSE)\n\n########################################################################\n\n#Checking how many males & females were there\nsummary(train$Sex)\n\n#Getting how many males/females died/survived\ntable(train$Sex, train$Survived)\n\n#proportions of above. (every entry is divided by the total num of passengers)\nprop.table(table(train$Sex, train$Survived))\n\n#proportions according to row by adding ,1 in the function. 2 is Column.\n#with rounding to 2 decimal places\nround(prop.table(table(train$Sex, train$Survived),1),2)\n\n#adding 0's tot he whole column the easy way\ntest$Survived <- 0\n\n#adding 1 to the subset of passengers where variable 'sex' is equalto 'female'\ntest$Survived[test$Sex == 'Female'] <- 1\n\n#Creating file with prediction that all females survived!\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)\nwrite.csv(submit, file = \"allfemalessurvived.csv\", row.names = FALSE)\n\n#############################################################################\n\n#summary() gives the summary stats of the argument\nsummary(train$Age)\n\n# Creating a new variable \"Child\" where the age is less than 18\ntrain$Child <- 0\ntrain$Child [train$Age < 18] <- 1\n\n\n# Creating a table with both Gender and Age to see Survival proportaitions\n# of each subset\n# We are using the aggrigate() function.\naggregate(Survived ~ Child + Sex, data = train, FUN = sum)\n\n# Creating a table with the proportions \naggregate(Survived ~ Child + Sex, data = train, FUN = function(x) {sum(x)/length(x)})\n\n#Creating bins for the fares (<10, 10-20, 20, 20-30, >30)\ntrain$Fare2 <- '30+'\ntrain$Fare2 [train$Fare >= 20 & train$Fare <30] <- '20-30'\ntrain$Fare2 [train$Fare >= 10 & train$Fare <20] <- '10-20'\ntrain$Fare2 [train$Fare <10] <- '<10'\n\n# Running aggrigate() with fare, sex and pclass as subsets\naggregate(Survived ~ Fare2 + Pclass + Sex, data = train, FUN = function(x) {sum(x)/length(x)})\n# We can see from the output that females who paid more than $20 and were class 3 \n# had less chance of survival\n# Males more or less had less chance regardless\n\n# Creating .csv with new prediction that all females survived except for the ones\n# who paid more than 20 and were Pclass 3\ntest$Survived[test$Sex==\"female\"] <- 1\ntest$Survived[test$Sex==\"female\" & test$Pclass==3 & (test$Fare=='20-30' | test$Fare=='30+')] <- 0\n\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)\nwrite.csv(submit, file=\"pclass3and20plusFDied.csv\", row.names = FALSE)\n\n#################################################################################\n\n# DECISION TREES\n# Getting rpart() which is for Recursive Partitioning and Regression Trees\nlibrary(rpart)\n\n# Format of rpart() is similar to aggrigate()\nfit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + \n                 Embarked, data=train, method=\"class\")\n\n# Examining the Tree\nplot(fit)\ntext(fit)\n\n# Installing and loading better R packages for visualizing\ninstall.packages('rattle')\ninstall.packages('rpart.plot')\ninstall.packages('RColorBrewer')\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\n\n# Better plot\nfancyRpartPlot(fit)\n\n# using predict function\nPrediction <- predict(fit,test,type=\"class\")\n\n# writing result to .csv\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"myfirstdtree.csv\", row.names = FALSE)\n\n###########################################################################\n\n# Increasing the complexity of the dtree (?rpart.control)\nfit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,\n             method=\"class\", control=rpart.control(minsplit=2, cp=0))\nfancyRpartPlot(fit)\n\n# Using prediction function\nPrediction <- predict(fit, test, type=\"class\")\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"complexdtree.csv\", row.names = FALSE)\n\n#########################################################################\n\n# Manually adjusting the complexity of the dtree\nfit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,\n             method=\"class\", control=rpart.control(minsplit=20, maxdepth=8))\nnew.fit <- prp(fit,snip=TRUE)$obj\nfancyRpartPlot(new.fit)\n\n# Using prediction function\nPrediction <- predict(new.fit, test, type=\"class\")\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"newcomplexdtree.csv\", row.names = FALSE)\n\n#########################################################################\n\n# Using FEATURE ENGINEERING\n#placing NA in test survived column and combining train and test dataframes\ntest$Survived <- NA\ntrain$Child <- NULL #removing other columns that we created\ntrain$Fare2 <- NULL\ncombi <- rbind(train, test)\n\n# changing Name column to string\ncombi$Name <- as.character(combi$Name)\n\n# getting only the title in a column\ncombi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})\n\n#stripping off the space before the title\ncombi$Title <- sub(' ', '', combi$Title)\n\n# combining some titles together\ncombi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'\ncombi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'\ncombi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'\n\n# changing Title back to Factor\ncombi$Title <- factor(combi$Title)\n\n# Combining SibSp and Parch together into familysize\ncombi$FamilySize <- combi$SibSp + combi$Parch + 1\n\n# Combining Lastname and family size in one column\ncombi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})\ncombi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep=\"\")\n\ntable(combi$FamilyID)\n\n#trying to move small family sizes in one \ncombi$FamilyID[combi$FamilySize <= 2] <- 'Small'\n\ntable(combi$FamilyID)\n\n# further cleaning to Remove the ones that are left \n# saving table above to dataframe\nfamIDs <- data.frame(table(combi$FamilyID))\nView(famIDs)\n\n# subset this dataframe to show only those unexpectedly small FamilyID groups\nfamIDs <- famIDs[famIDs$Freq <= 2,]\n\n#We then need to overwrite any family IDs in our dataset for groups \n# that were not correctly identified and finally convert it to a factor:\ncombi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'\ncombi$FamilyID <- factor(combi$FamilyID)\n\n# Breaking the test and train \ntrain <- combi[1:891,]\ntest <- combi[892:1309,]\n\n# New prediction\nfit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare \n             + Embarked + Title + FamilySize + FamilyID,data = train, \n             method=\"class\")\n\nfancyRpartPlot(fit)\n\n# \nPrediction <- predict(fit, test, type=\"class\")\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"featureengineering.csv\", row.names = FALSE)\n\n##################################################################\n\n# Making a prediction without the FamilyID \nfit <- rpart(Survived ~ Sex + Pclass + Fare + Title + Age + Embarked + \n                 FamilySize, data = train, method = \"class\")\nfancyRpartPlot(fit)\nPrediction <- predict(fit, test, type=\"class\")\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"featureengnofamid.csv\", row.names = FALSE)\n\n# IMPROVED SCORE THEN WITH FAMILYID!!!\n\n\n##################################################################\n\n# RANDOM FORESTS\n# Missing values have to be cleaned because unlike rpart(), Ramdom Forests\n# cannot deal with missing, NA or NAN values\n\n# Random Forests also cannot deal with factors that have more than 32 levels\n# FamilyID has 61 factors which will have to be pruned. We can also use\n# unclass() to treat them as continous variables\n\n# using summary() to find NA\nsummary(combi)\n\n# using rpart() to predict missing age values\nAgefit <- rpart(Age~Sex+Pclass+Fare+SibSp+Parch+Fare+Embarked+FamilySize, data=combi[!is.na(combi$Age),],method=\"anova\")\ncombi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])\n\n# using which() to fing the 2 persons with no Embarked value.\n# we will replace the missing value with S because a large\n# majority emparked from there. Also changing it back to factor after adding\n# the 2 strings \"s\"\nwhich(combi$Embarked == \"\")\ncombi$Embarked[c(62,830)] <- \"S\"\ncombi$Embarked <- factor(combi$Embarked)\n\n# Doing the same for Fare\nwhich(is.na(combi$Fare))\ncombi$Fare[1044] <- median(combi$Fare, na.rm = TRUE)\n\n#lowering the total factor levels in FamilyID\ncombi$FamilyID2 <- combi$FamilyID\ncombi$FamilyID2 <- as.character(combi$FamilyID2)\ncombi$FamilyID2[combi$FamilySize <= 3] <- 'Small'\ncombi$FamilyID2 <- factor(combi$FamilyID2)\n\n# Breaking the test and train \ntrain <- combi[1:891,]\ntest <- combi[892:1309,]\n\n\n#installing randomForest package\ninstall.packages(\"randomForest\")\nlibrary(randomForest)\n\n# Because the process has the two sources of randomness that we discussed \n# earlier, it is a good idea to set the random seed in R before you begin.\n# This makes your results reproducible next time you load the code up, \n# otherwise you can get different classifications for each run.\n# The number is not important but using the same number each time is!\nset.seed(415)\n\n#Random Forest function. Instead of specifying method=\"class\" as with rpart, \n#we force the model to predict our classification by temporarily changing our\n#target variable to a factor with only two levels using as.factor(). \n#The importance=TRUE argument allows us to inspect variable importance \n#as we'll see, and the ntree argument specifies how many trees we want to grow.\n\n#If you were working with a larger dataset you may want to reduce the \n#number of trees, at least for initial exploration, or restrict the \n#complexity of each tree using nodesize as well as reduce the number \n#of rows sampled with sampsize. You can also override the default number \n#of variables to choose from with mtry, but the default is the square root \n#of the total number available and that should work just fine. \nfit <- randomForest(as.factor(Survived) ~ Pclass+Sex+Age+SibSp+Parch+\n                        Fare+Embarked+Title+FamilySize+FamilyID2, \n                    data=train, importance=TRUE, ntree=2000)\n\n#roughly 37% of our rows would be left out? Well Random Forests doesn't \n#just waste those \"out-of-bag\" (OOB) observations, it uses them to see how \n#well each tree performs on unseen data. It's almost like a bonus test set \n#to determine your model's performance on the fly. There's two types of \n#importance measures shown above. The accuracy one tests to see how worse \n#the model performs without each variable, so a high decrease in accuracy \n#would be expected for very predictive variables. The Gini one digs into \n#the mathematics behind decision trees, but essentially measures how pure \n#the nodes are at the end of the tree. Again it tests to see the result \n#if each variable is taken out and a high score means the variable was \n#important.\nvarImpPlot(fit)\n\n#Title was most important variable in both\n\nPrediction <- predict(fit, test)\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"firstforest.csv\", row.names = FALSE)\n\n#NOT THE BEST SUBMISSION!!!\n\n####################################################################\n\n#There's more than one ensemble model. Let's try a forest of \n# conditional inference trees. They make their decisions in slightly \n# different ways, using a statistical test rather than a purity measure, \n#but the basic construction of each tree is fairly similar\n\ninstall.packages('party')\ninstall.packages(\"sandwich\")\nlibrary(party)\nlibrary(sandwich)\nset.seed(415)\n\nfit <- cforest(as.factor(Survived)~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+FamilySize+FamilyID, data=train, controls=cforest_unbiased(ntree=2000, mtry=3))\n\n#Conditional inference trees are able to handle factors with more levels \n#than Random Forests can, so let's go back to out original version of \n#FamilyID. You may have also noticed a few new arguments. Now we have to \n#specify the number of trees inside a more complicated command, as arguments\n#are passed to cforest differently. We also have to manually set the number \n#of variables to sample at each node as the default of 5 is pretty high for \n#our dataset.\n\n#The prediction function requires a few extra nudges for \n# conditional inference forests\n\nPrediction <- predict(fit, test, OOB=TRUE, type = \"response\")\n\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"firstCforest.csv\", row.names = FALSE)\n\n# BEST PREDICTION SO FAR!!!!\n\n#########################################################################\n\n# Trying without FamilyID\nlibrary(party)\nlibrary(sandwich)\nset.seed(415)\n\nfit <- cforest(as.factor(Survived)~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+FamilySize+FamilyID, data=train, controls=cforest_unbiased(ntree=2000, mtry=3))\nPrediction <- predict(fit, test, OOB=TRUE, type = \"response\")\n\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"CforestwithoutfamID.csv\", row.names = FALSE)\n\n# SAME SCORE!!!\n\n###########################################################################\n# Putting Cabins into Bins \n\nodd <- seq(1,200,2)\n\neven <- seq(2,200,2)\n\ncombi$Cabin2 <- combi$Cabin\n\ncombi$Cabin2<-as.character(combi$Cabin2)\n\n# Deck A, Front:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[2:18],function(x) {paste(\"A\",x,sep=\"\")})]<-\"AFR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[2:18],function(x) {paste(\"A\",x,sep=\"\")})]<-\"AFL\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[1:2],function(x) {paste(\"A\",x,sep=\"\")})]<-\"AFM\"\n\n# Deck B, Front:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[2:25],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BFR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[2:25],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BFL\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[c(1,2,50)],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BFM\"\n\n# Deck B, Mid:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[26:49],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BMR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[26:49],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BML\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[c(99,100,101,102)],function(x) {paste(\"B\",x,sep=\"\")})]<-\"BMM\"\n\n# Deck C, Front:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[3:18],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CFR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[3:16],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CFL\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(1:4,odd[21:27],even[24:30]),function(x) {paste(\"C\",x,sep=\"\")})]<-\"CFM\"\n\n# Deck C, Mid:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[c(19:20,29:33)],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CMR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[c(14:23,31:37)],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CML\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[c(odd[48:58],even[52:62])],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CMM\"\n\n# Deck C, Back:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[35:47],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CBR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[38:51],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CBL\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[c(odd[59:64],even[63:78])],function(x) {paste(\"C\",x,sep=\"\")})]<-\"CBM\"\n\n# Deck D, Front:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd[3:19],function(x) {paste(\"D\",x,sep=\"\")})]<-\"DFR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even[3:19],function(x) {paste(\"D\",x,sep=\"\")})]<-\"DFL\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(numbers[c(numbers[1:4],numbers[40:50])],function(x) {paste(\"D\",x,sep=\"\")})]<-\"DFM\"\n\n# Deck D, Back:\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(odd[26:44],odd[64:69]),function(x) {paste(\"D\",x,sep=\"\")})]<-\"DBR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(even[26:44],even[64:68]),function(x) {paste(\"D\",x,sep=\"\")})]<-\"DBL\"\n\n# Deck E, Front, Middle & Back:\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(1:44),function(x) {paste(\"E\",x,sep=\"\")})]<-\"EFR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(45:90),function(x) {paste(\"E\",x,sep=\"\")})]<-\"EMR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(c(91:167),function(x) {paste(\"E\",x,sep=\"\")})]<-\"EBR\"\n\n# Deck F, Back:\ncombi$Cabin2[combi$Cabin2 %in% lapply(odd,function(x) {paste(\"F\",x,sep=\"\")})]<-\"FBR\"\ncombi$Cabin2[combi$Cabin2 %in% lapply(even,function(x) {paste(\"F\",x,sep=\"\")})]<-\"FBL\"\n\n# All Empty Cells as \"\"\ncombi$Cabin2[combi$Cabin2 %in% \"\"] <- \"MISSING\"\n\n# Cleaning multiple values\ncombi$Cabin2[c(680,873,1235)] <- \"BMR\"\ncombi$Cabin2[c(312,743,916,956,1034)] <- \"BMR\"\ncombi$Cabin2[c(119,300,1076)] <- \"BML\"\ncombi$Cabin2[c(1264)] <- \"BML\"\ncombi$Cabin2[c(790)] <- \"BML\"\ncombi$Cabin2[c(391,436,764,803)] <- \"BML\"\ncombi$Cabin2[c(298,306,499,1198)] <- \"CFL\"\ncombi$Cabin2[c(28,89,342,439,945,961)] <- \"CFR\"\ncombi$Cabin2[c(973,1006)] <- \"CMM\"\ncombi$Cabin2[c(701,1094)] <- \"CML\"\ncombi$Cabin2[c(293,328,474,1193)] <- \"DBR\"\ncombi$Cabin2[c(98,1242)] <- \"DFL\"\ncombi$Cabin2[c(1263)] <- \"EFR\"\ncombi$Cabin2[c(1180,1213,129)] <- \"EMR\"\ncombi$Cabin2[c(700,949,76,716,11,206,252,395,1009)] <- \"G\"\n\n# Back to Factor\ncombi$Cabin2<-as.factor(combi$Cabin2)\n\nlibrary(party)\nlibrary(sandwich)\nset.seed(415)\n\n\n\ntrain <- combi[1:891,]\ntest <- combi[892:1309,]\n\n\nfit <- cforest(as.factor(Survived)~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+FamilySize+FamilyID+Cabin2, data=train, controls=cforest_unbiased(ntree=2000, mtry=3))\n\nPrediction <- predict(fit, test, OOB=TRUE, type = \"response\")\n\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"CforestCabins.csv\", row.names = FALSE)\n\n#BEST SUBMISSION SO FAR!!!!!\n##################################################################\nset.seed(415)\n\nfit <- cforest(as.factor(Survived)~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+FamilySize+FamilyID+Cabin2, data=train, controls=cforest_unbiased(ntree=2000, mtry=3))\n\nPrediction <- predict(fit, test, OOB=TRUE, type = \"response\")\n\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\nwrite.csv(submit, file = \"CForestCabinNoFID.csv\", row.names = FALSE)\n# Ticket into bins\n\n# Have to try this -> Thanks for sharing, it's a great tutorial. \n#Using some of your hints I've just improved my model and reached position \n#100 (0.81818).\n\n#One thing you missed: You can get the number of persons travelling with \n#a ticket, grouping data by ticket number. Then, divide ticket fare by the \n#number of persons travelling with the ticket and you get the fare per person.\n#Using that value I got a better accuracy than using the total fare.\n\n#I guess I could improve my model even more by digging deeper and finding \n#relationships between people travelling together using ticket numbers, \n#family surnames, and single surnames of the spouses (between parenthesis \n#in the 'Name' field), maybe I'll give it a try\n\ndupliticket<-c(which(duplicated(x = combi$Ticket)==TRUE))\ncombi$Ticket[c(dupliticket)]\nlevels(combi$Ticket)\nsapply(combi$Ticket,function(x){if(x==as.character(levels(dupliticket)) print(x)})\n",
    "created" : 1445301847635.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3526126281",
    "id" : "9B890296",
    "lastKnownWriteTime" : 1431109157,
    "path" : "~/Projects/Projects/Kaggle Competitions/Titanic/titanic.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}